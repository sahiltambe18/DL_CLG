{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.4500000097644881\n",
      "Epoch 2/100, Loss: 0.45000002174476367\n",
      "Epoch 3/100, Loss: 0.45000003869630106\n",
      "Epoch 4/100, Loss: 0.4500000609061232\n",
      "Epoch 5/100, Loss: 0.4500000887589745\n",
      "Epoch 6/100, Loss: 0.45000012274408774\n",
      "Epoch 7/100, Loss: 0.4500001634638823\n",
      "Epoch 8/100, Loss: 0.45000021164474235\n",
      "Epoch 9/100, Loss: 0.450000268150069\n",
      "Epoch 10/100, Loss: 0.4500003339958349\n",
      "Epoch 11/100, Loss: 0.45000041036891814\n",
      "Epoch 12/100, Loss: 0.4500004986485429\n",
      "Epoch 13/100, Loss: 0.45000060043120677\n",
      "Epoch 14/100, Loss: 0.45000071755954063\n",
      "Epoch 15/100, Loss: 0.4500008521556137\n",
      "Epoch 16/100, Loss: 0.4500010066592748\n",
      "Epoch 17/100, Loss: 0.45000118387221305\n",
      "Epoch 18/100, Loss: 0.45000138700851716\n",
      "Epoch 19/100, Loss: 0.4500016197526266\n",
      "Epoch 20/100, Loss: 0.45000188632570187\n",
      "Epoch 21/100, Loss: 0.4500021915615771\n",
      "Epoch 22/100, Loss: 0.4500025409936306\n",
      "Epoch 23/100, Loss: 0.45000294095409726\n",
      "Epoch 24/100, Loss: 0.4500033986875491\n",
      "Epoch 25/100, Loss: 0.45000392248052307\n",
      "Epoch 26/100, Loss: 0.450004521809537\n",
      "Epoch 27/100, Loss: 0.45000520751004996\n",
      "Epoch 28/100, Loss: 0.4500059919692639\n",
      "Epoch 29/100, Loss: 0.4500068893460662\n",
      "Epoch 30/100, Loss: 0.45000791582184824\n",
      "Epoch 31/100, Loss: 0.45000908988643884\n",
      "Epoch 32/100, Loss: 0.4500104326639606\n",
      "Epoch 33/100, Loss: 0.45001196828404366\n",
      "Epoch 34/100, Loss: 0.45001372430454994\n",
      "Epoch 35/100, Loss: 0.4500157321927605\n",
      "Epoch 36/100, Loss: 0.45001802787287354\n",
      "Epoch 37/100, Loss: 0.45002065234866384\n",
      "Epoch 38/100, Loss: 0.4500236524112681\n",
      "Epoch 39/100, Loss: 0.450027081443312\n",
      "Epoch 40/100, Loss: 0.45003100033196913\n",
      "Epoch 41/100, Loss: 0.4500354785050737\n",
      "Epoch 42/100, Loss: 0.4500405951060999\n",
      "Epoch 43/100, Loss: 0.4500464403256762\n",
      "Epoch 44/100, Loss: 0.4500531169093357\n",
      "Epoch 45/100, Loss: 0.450060741863432\n",
      "Epoch 46/100, Loss: 0.4500694483835516\n",
      "Epoch 47/100, Loss: 0.4500793880323628\n",
      "Epoch 48/100, Loss: 0.4500907331966284\n",
      "Epoch 49/100, Loss: 0.4501036798560735\n",
      "Epoch 50/100, Loss: 0.4501184506999328\n",
      "Epoch 51/100, Loss: 0.45013529863025603\n",
      "Epoch 52/100, Loss: 0.4501545106943978\n",
      "Epoch 53/100, Loss: 0.4501764124925039\n",
      "Epoch 54/100, Loss: 0.4502013731091365\n",
      "Epoch 55/100, Loss: 0.4502298106213793\n",
      "Epoch 56/100, Loss: 0.45026219823866853\n",
      "Epoch 57/100, Loss: 0.45029907113208323\n",
      "Epoch 58/100, Loss: 0.45034103401264886\n",
      "Epoch 59/100, Loss: 0.4503887695191498\n",
      "Epoch 60/100, Loss: 0.4504430474756827\n",
      "Epoch 61/100, Loss: 0.4505047350773516\n",
      "Epoch 62/100, Loss: 0.45057480805867917\n",
      "Epoch 63/100, Loss: 0.450654362892981\n",
      "Epoch 64/100, Loss: 0.4507446300615266\n",
      "Epoch 65/100, Loss: 0.4508469884181389\n",
      "Epoch 66/100, Loss: 0.4509629806572083\n",
      "Epoch 67/100, Loss: 0.45109432987009973\n",
      "Epoch 68/100, Loss: 0.45124295714572166\n",
      "Epoch 69/100, Loss: 0.45141100013469043\n",
      "Epoch 70/100, Loss: 0.4516008324521038\n",
      "Epoch 71/100, Loss: 0.4518150837405493\n",
      "Epoch 72/100, Loss: 0.45205666015179985\n",
      "Epoch 73/100, Loss: 0.4523287649320656\n",
      "Epoch 74/100, Loss: 0.4526349187113442\n",
      "Epoch 75/100, Loss: 0.4529789790024042\n",
      "Epoch 76/100, Loss: 0.45336515830990587\n",
      "Epoch 77/100, Loss: 0.45379804013648667\n",
      "Epoch 78/100, Loss: 0.45428259205268484\n",
      "Epoch 79/100, Loss: 0.4548241748748081\n",
      "Epoch 80/100, Loss: 0.4554285468741915\n",
      "Epoch 81/100, Loss: 0.45610186182910234\n",
      "Epoch 82/100, Loss: 0.4568506596349969\n",
      "Epoch 83/100, Loss: 0.4576818481197364\n",
      "Epoch 84/100, Loss: 0.45860267467923227\n",
      "Epoch 85/100, Loss: 0.4596206863687491\n",
      "Epoch 86/100, Loss: 0.46074367716951664\n",
      "Epoch 87/100, Loss: 0.4619796213134115\n",
      "Epoch 88/100, Loss: 0.46333659180325204\n",
      "Epoch 89/100, Loss: 0.46482266362349245\n",
      "Epoch 90/100, Loss: 0.46644580160251076\n",
      "Epoch 91/100, Loss: 0.46821373346416\n",
      "Epoch 92/100, Loss: 0.47013380928583276\n",
      "Epoch 93/100, Loss: 0.47221284934639984\n",
      "Epoch 94/100, Loss: 0.47445698317244434\n",
      "Epoch 95/100, Loss: 0.4768714834360023\n",
      "Epoch 96/100, Loss: 0.47946059917111206\n",
      "Epoch 97/100, Loss: 0.4822273935001132\n",
      "Epoch 98/100, Loss: 0.48517359162808715\n",
      "Epoch 99/100, Loss: 0.4882994452084427\n",
      "Epoch 100/100, Loss: 0.4916036192431054\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define activation functions and their derivatives\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def mse_loss(y_pred, y_true):\n",
    "    return np.mean((y_pred - y_true)**2)\n",
    "\n",
    "# Initialize the RNN parameters\n",
    "input_size = 1      # Input feature size (e.g., a single value in a sequence)\n",
    "hidden_size = 16    # Number of hidden units\n",
    "output_size = 1     # Output size (single value prediction)\n",
    "sequence_length = 10 # Length of each input sequence\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Weight matrices\n",
    "Wxh = np.random.randn(hidden_size, input_size) * 0.01  # Input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01 # Hidden to hidden\n",
    "Why = np.random.randn(output_size, hidden_size) * 0.01 # Hidden to output\n",
    "\n",
    "# Biases\n",
    "bh = np.zeros((hidden_size, 1))\n",
    "by = np.zeros((output_size, 1))\n",
    "\n",
    "# Forward pass through the RNN for a sequence\n",
    "def forward(x_seq):\n",
    "    h_seq = np.zeros((sequence_length, hidden_size, 1))  # Stores hidden states\n",
    "    y_seq = np.zeros((sequence_length, output_size, 1))  # Stores outputs\n",
    "\n",
    "    for t in range(sequence_length):\n",
    "        x = x_seq[t].reshape(-1, 1)  # Get current time step input and reshape for matrix operations\n",
    "        h_prev = h_seq[t - 1] if t > 0 else np.zeros((hidden_size, 1))  # Use previous hidden state\n",
    "        h = tanh(np.dot(Wxh, x) + np.dot(Whh, h_prev) + bh)  # Compute hidden state\n",
    "        y = np.dot(Why, h) + by                               # Compute output\n",
    "\n",
    "        h_seq[t] = h  # Save hidden state\n",
    "        y_seq[t] = y  # Save output\n",
    "\n",
    "    return y_seq, h_seq\n",
    "\n",
    "# Backpropagation Through Time (BPTT) for the RNN\n",
    "def backward(x_seq, y_true_seq, y_seq, h_seq):\n",
    "    global Wxh, Whh, Why, bh, by\n",
    "\n",
    "    # Gradients initialization\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dh_next = np.zeros((hidden_size, 1))  # Gradient of the next time step's hidden state\n",
    "\n",
    "    for t in reversed(range(sequence_length)):\n",
    "        dy = y_seq[t] - y_true_seq[t].reshape(-1, 1)  # Output error\n",
    "        dWhy += np.dot(dy, h_seq[t].T)\n",
    "        dby += dy\n",
    "\n",
    "        dh = np.dot(Why.T, dy) + dh_next  # Backpropagate into hidden state\n",
    "        dh_raw = dh * tanh_derivative(h_seq[t])  # Apply tanh derivative\n",
    "        dbh += dh_raw\n",
    "        dWxh += np.dot(dh_raw, x_seq[t].reshape(1, -1))\n",
    "        if t > 0:\n",
    "            dWhh += np.dot(dh_raw, h_seq[t - 1].T)\n",
    "        dh_next = np.dot(Whh.T, dh_raw)\n",
    "\n",
    "    # Update weights and biases using gradient descent\n",
    "    for param, dparam in zip([Wxh, Whh, Why, bh, by], [dWxh, dWhh, dWhy, dbh, dby]):\n",
    "        param -= learning_rate * dparam\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    # Generate a dummy input sequence and target\n",
    "    x_seq = np.sin(np.linspace(0, 2 * np.pi, sequence_length))  # Example sequence (sinusoidal)\n",
    "    y_true_seq = np.roll(x_seq, -1)  # Target is the sequence shifted by one step\n",
    "    x_seq = x_seq.reshape(sequence_length, 1)  # Reshape for processing\n",
    "    y_true_seq = y_true_seq.reshape(sequence_length, 1)\n",
    "\n",
    "    # Forward pass\n",
    "    y_seq, h_seq = forward(x_seq)\n",
    "\n",
    "    # Calculate and print loss\n",
    "    loss = mse_loss(y_seq, y_true_seq)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss}\")\n",
    "\n",
    "    # Backward pass\n",
    "    backward(x_seq, y_true_seq, y_seq, h_seq)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
