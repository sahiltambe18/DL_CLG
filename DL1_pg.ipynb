{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eF-3JfJd-2mE",
        "outputId": "612f1828-418a-41e8-da07-9a0058e383cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n"
          ]
        }
      ],
      "source": [
        "# TensorFlow Implementation\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "\n",
        "# Generate some sample data\n",
        "np.random.seed(42)\n",
        "X_train = np.random.rand(1000, 10)\n",
        "y_train = np.sum(X_train, axis=1) > 5\n",
        "\n",
        "# TensorFlow Model\n",
        "def create_tf_model():\n",
        "    model = models.Sequential([\n",
        "        layers.Dense(64, activation='relu', input_shape=(10,)),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                 loss='binary_crossentropy',\n",
        "                 metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Train TensorFlow model\n",
        "tf_model = create_tf_model()\n",
        "tf_history = tf_model.fit(X_train, y_train,\n",
        "                         epochs=10,\n",
        "                         batch_size=32,\n",
        "                         validation_split=0.2,\n",
        "                         verbose=0)\n",
        "\n",
        "# PyTorch Implementation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_torch = torch.FloatTensor(X_train)\n",
        "y_torch = torch.FloatTensor(y_train)\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = TensorDataset(X_torch, y_torch)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# PyTorch Model\n",
        "class PyTorchModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PyTorchModel, self).__init__()\n",
        "        self.layer1 = nn.Linear(10, 64)\n",
        "        self.layer2 = nn.Linear(64, 32)\n",
        "        self.layer3 = nn.Linear(32, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.sigmoid(self.layer3(x))\n",
        "        return x\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "torch_model = PyTorchModel()\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(torch_model.parameters())\n",
        "\n",
        "# Training loop for PyTorch\n",
        "def train_pytorch_model(model, dataloader, criterion, optimizer, num_epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels.unsqueeze(1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "# Train PyTorch model\n",
        "train_pytorch_model(torch_model, dataloader, criterion, optimizer)\n",
        "\n",
        "# Example predictions\n",
        "def make_predictions():\n",
        "    # Generate test data\n",
        "    X_test = np.random.rand(5, 10)\n",
        "\n",
        "    # TensorFlow predictions\n",
        "    tf_preds = tf_model.predict(X_test)\n",
        "\n",
        "    # PyTorch predictions\n",
        "    torch_model.eval()\n",
        "    with torch.no_grad():\n",
        "        torch_preds = torch_model(torch.FloatTensor(X_test))\n",
        "\n",
        "    return X_test, tf_preds, torch_preds.numpy()\n",
        "\n",
        "# Make and display predictions\n",
        "X_test, tf_preds, torch_preds = make_predictions()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. What is a tensor?\n",
        "A tensor is a fundamental data structure used in deep learning - it's a generalization of vectors and matrices to potentially higher dimensions. Think of it as:\n",
        "- Scalar: 0-dimensional tensor (single number)\n",
        "- Vector: 1-dimensional tensor (list of numbers)\n",
        "- Matrix: 2-dimensional tensor (table of numbers)\n",
        "- N-dimensional array: Higher dimensional tensor\n",
        "\n",
        "Example:\n",
        "```python\n",
        "# Scalar (0D tensor)\n",
        "scalar = tf.constant(5)\n",
        "\n",
        "# Vector (1D tensor)\n",
        "vector = tf.constant([1, 2, 3, 4])\n",
        "\n",
        "# Matrix (2D tensor)\n",
        "matrix = tf.constant([[1, 2], [3, 4]])\n",
        "\n",
        "# 3D tensor\n",
        "tensor_3d = tf.constant([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
        "```\n",
        "\n",
        "2. Dimensions and Ranks in TensorFlow:\n",
        "- Rank: Number of dimensions in a tensor\n",
        "- Shape: Length of each dimension\n",
        "\n",
        "Examples:\n",
        "```python\n",
        "# Rank 0 (scalar): shape []\n",
        "t0 = tf.constant(42)\n",
        "\n",
        "# Rank 1 (vector): shape [3]\n",
        "t1 = tf.constant([1, 2, 3])\n",
        "\n",
        "# Rank 2 (matrix): shape [2, 3]\n",
        "t2 = tf.constant([[1, 2, 3],\n",
        "                 [4, 5, 6]])\n",
        "\n",
        "# Rank 3: shape [2, 2, 2]\n",
        "t3 = tf.constant([[[1, 2], [3, 4]],\n",
        "                 [[5, 6], [7, 8]]])\n",
        "```\n",
        "\n",
        "3. Building Models in Keras:\n",
        "There are three main ways to build models in Keras:\n",
        "\n",
        "a. Sequential API (simplest):\n",
        "```python\n",
        "from tensorflow.keras import Sequential, layers\n",
        "\n",
        "model = Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(784,)),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "```\n",
        "\n",
        "b. Functional API (more flexible):\n",
        "```python\n",
        "from tensorflow.keras import Model, Input\n",
        "\n",
        "inputs = Input(shape=(784,))\n",
        "x = layers.Dense(64, activation='relu')(inputs)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "outputs = layers.Dense(10, activation='softmax')(x)\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "```\n",
        "\n",
        "c. Subclassing (most flexible):\n",
        "```python\n",
        "class CustomModel(Model):\n",
        "    def __init__(self):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.dense1 = layers.Dense(64, activation='relu')\n",
        "        self.dropout = layers.Dropout(0.2)\n",
        "        self.dense2 = layers.Dense(10, activation='softmax')\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dropout(x)\n",
        "        return self.dense2(x)\n",
        "```\n",
        "\n",
        "4. Function Operations in Theano:\n",
        "Theano (though now deprecated) introduced several key concepts still used in modern frameworks:\n",
        "- Symbolic Variables: Placeholders for data\n",
        "- Computational Graphs: Operations arranged in a directed graph\n",
        "- Function Compilation: Converting symbolic expressions to efficient code\n",
        "\n",
        "Example of Theano-style operations (modern equivalent in TensorFlow):\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define variables\n",
        "x = tf.Variable(initial_value=3.0)\n",
        "y = tf.Variable(initial_value=2.0)\n",
        "\n",
        "# Define operation\n",
        "@tf.function  # Similar to Theano's function compilation\n",
        "def compute_z(x, y):\n",
        "    return tf.square(x) + y\n",
        "\n",
        "# Execute operation\n",
        "z = compute_z(x, y)\n",
        "```\n",
        "\n",
        "5. PyTorch vs TensorFlow Differences:\n",
        "\n",
        "Key Differences:\n",
        "1. Dynamic vs Static Graphs:\n",
        "```python\n",
        "# PyTorch (Dynamic)\n",
        "class PyTorchModel(nn.Module):\n",
        "    def forward(self, x):\n",
        "        # Can modify behavior at runtime\n",
        "        if self.training:\n",
        "            return x * 2\n",
        "        return x\n",
        "\n",
        "# TensorFlow (Static with @tf.function)\n",
        "@tf.function\n",
        "def tensorflow_model(x):\n",
        "    # Graph is fixed after first run\n",
        "    return x * 2\n",
        "```\n",
        "\n",
        "2. Eager Execution:\n",
        "```python\n",
        "# PyTorch (Always eager by default)\n",
        "x = torch.tensor([1, 2, 3])\n",
        "y = x + 2  # Immediate execution\n",
        "\n",
        "# TensorFlow (Can switch between eager and graph)\n",
        "tf.config.run_functions_eagerly(True)  # Enable eager mode\n",
        "x = tf.constant([1, 2, 3])\n",
        "y = x + 2  # Immediate execution\n",
        "```\n",
        "\n",
        "3. API Design:\n",
        "```python\n",
        "# PyTorch (Object-Oriented)\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(10, 5),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(5, 1)\n",
        ")\n",
        "\n",
        "# TensorFlow (More functional)\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(5, activation='relu', input_shape=(10,)),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "```\n",
        "\n",
        "The main philosophical differences are:\n",
        "- PyTorch is more Python-native and research-friendly\n",
        "- TensorFlow is more production-focused with better deployment tools\n",
        "- PyTorch has a more imperative style\n",
        "- TensorFlow has better visualization tools (TensorBoard)\n",
        "- PyTorch has better debugging capabilities due to its dynamic nature\n",
        "\n"
      ],
      "metadata": {
        "id": "H1AkzH7PA9RS"
      }
    }
  ]
}