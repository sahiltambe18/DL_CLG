{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eF-3JfJd-2mE",
        "outputId": "612f1828-418a-41e8-da07-9a0058e383cc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n"
          ]
        }
      ],
      "source": [
        "# TensorFlow Implementation\n",
        "#  TensorFlow library, a powerful open-source library developed by Google for machine learning and deep learning tasks.\n",
        "import tensorflow as tf\n",
        "# keras-> tensorflow api used to buid and train the neural network model\n",
        "# 1)layers -> like dense layer , convolutional layer, recurrent layer\n",
        "# 2)models -> Provides a structure for building and managing a neural network model in Keras.\n",
        "from tensorflow.keras import layers, models \n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Generate some sample data\n",
        "np.random.seed(42)\n",
        "X_train = np.random.rand(1000, 10)\n",
        "y_train = np.sum(X_train, axis=1) > 5\n",
        "\n",
        "# TensorFlow Model\n",
        "# models.Sequential creates a sequential model where layers are stacked in order\n",
        "def create_tf_model():\n",
        "    model = models.Sequential([\n",
        "    #his is a Dense (fully connected) layer with 64 units (neurons) and a ReLU (Rectified Linear Unit) activation function.\n",
        "    #The input_shape=(10,) specifies that the model expects each input to have 10 features, matching the shape of X_train.\n",
        "    #ReLU helps the network learn non-linear relationships.\n",
        "        layers.Dense(64, activation='relu', input_shape=(10,)),\n",
        "    # Dropout is a regularization technique where a fraction of neurons (20% in this case) are randomly \"dropped\" (ignored) during training\n",
        "        layers.Dropout(0.2),\n",
        "    # t learns more complex relationships\n",
        "        layers.Dense(32, activation='relu'),\n",
        "    # Since this model is set up for binary classification, the sigmoid function outputs a probability between 0 and 1\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    # The optimizer is like the engine that drives the learning process. \"Adam\" is a specific optimizer that helps the model adjust its internal settings (called weights) efficiently to minimize errors during training. It’s popular because it works well on many types of problems.\n",
        "    # The loss function is how we measure the model’s mistakes. Since this model is for a binary classification problem (predicting one of two classes, like True/False), we use \"binary cross-entropy\" as the loss function. This function calculates how close the model’s predictions are to the actual answers. The goal is to make this loss as small as possible, which means the model is learning well.\n",
        "    # metric tell us how accurate our model predict it gives us the percentage of correct orediction\n",
        "    model.compile(optimizer='adam',\n",
        "                 loss='binary_crossentropy',\n",
        "                 metrics=['accuracy'])\n",
        "    return model\n",
        "# his function builds a simple feed-forward neural network for binary classification with a dropout layer to prevent overfitting and a sigmoid output layer for probability-based binary predictions. The model is compiled with settings suitable for binary classification tasks\n",
        "\n",
        "# Train TensorFlow model\n",
        "tf_model = create_tf_model()\n",
        "#  epochs -> The model will go through the entire dataset 10 times\n",
        "# batch_size=32: -> The data is processed in \"batches\" of 32 samples at a time. \n",
        "# validation_split=0.2,-> Reserves 20% of X_train and y_train for validation (checking model performance on unseen data) while training on the remaining 80%.\n",
        "# verbose=0: Runs the training silently, without printing updates in the console. Setting verbose=1 will show progress for each epoch.\n",
        "tf_history = tf_model.fit(X_train, y_train,\n",
        "                         epochs=10,\n",
        "                         batch_size=32,\n",
        "                         validation_split=0.2,\n",
        "                         verbose=0)\n",
        "\n",
        "# PyTorch Implementation\n",
        "# import torch:\n",
        "\n",
        "# This imports the main PyTorch library, which provides the core functionalities for tensor operations and building neural networks.\n",
        "# import torch.nn as nn:\n",
        "\n",
        "# This imports the nn module from PyTorch, which contains classes and functions to build neural networks, including various layers, loss functions, and activation functions.\n",
        "# import torch.optim as optim:\n",
        "\n",
        "# This imports the optim module, which provides optimization algorithms (like SGD, Adam, etc.) to update the parameters of your neural network during training.\n",
        "# from torch.utils.data import TensorDataset, DataLoader:\n",
        "\n",
        "# This imports two classes:\n",
        "# TensorDataset: This is a dataset class that allows you to wrap your input tensors (features) and target tensors (labels) into a single dataset object. This makes it easier to iterate over the data in batches.\n",
        "# DataLoader: This is a utility that helps load your data in batches, shuffling the data if desired. It provides an iterable over the dataset, making it efficient to feed data to the model during training.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "\n",
        "# This indicates that the tensor will contain 32-bit floating-point numbers. \n",
        "# After this conversion, x_torch can be used directly in PyTorch for model training or inference.\n",
        "X_torch = torch.FloatTensor(X_train)\n",
        "y_torch = torch.FloatTensor(y_train)\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = TensorDataset(X_torch, y_torch)\n",
        "#  DataLoader simplifies the training loop, as you can directly iterate over batches of data in your training process, making the code cleaner and more efficient.\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# PyTorch Model\n",
        "# in pytorch all neural network model are inherited from nn.Module class\n",
        "# def __init__(self): -> This is the initializer (constructor) method for the class. It is called when you create an instance of PyTorchMode\n",
        "# |Inside this method, you define the layers and components of your neural network.\n",
        "# super(PyTorchModel, self).__init__()-> This line calls the constructor of the parent class (nn.Module). It ensures that the parent class is properly initialized, allowing your model to inherit the functionality provided by nn.Module\n",
        "class PyTorchModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PyTorchModel, self).__init__()\n",
        "         self.layer1 = nn.Linear(10, 64)     #It takes 10 input features (the size of the input data) and transforms them into 64 output features (neurons in this layer).\n",
        "        self.layer2 = nn.Linear(64, 32)      #This is typically used for binary classification tasks, where the model outputs a probability score\n",
        "        self.layer3 = nn.Linear(32, 1)\n",
        "        self.relu = nn.ReLU()     #ReLU is commonly used as an activation function in neural networks because it introduces non-linearity, allowing the model to learn complex patterns.\n",
        "        self.dropout = nn.Dropout(0.2)   #keep 20% for validation\n",
        "        self.sigmoid = nn.Sigmoid()   #This creates an instance of the Sigmoid activation function, which squashes the output to a range between 0 and 1.\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.sigmoid(self.layer3(x))\n",
        "        return x\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "torch_model = PyTorchModel()\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(torch_model.parameters())\n",
        "\n",
        "# Training loop for PyTorch\n",
        "def train_pytorch_model(model, dataloader, criterion, optimizer, num_epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels.unsqueeze(1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "# Train PyTorch model\n",
        "train_pytorch_model(torch_model, dataloader, criterion, optimizer)\n",
        "\n",
        "# Example predictions\n",
        "def make_predictions():\n",
        "    # Generate test data\n",
        "    X_test = np.random.rand(5, 10)\n",
        "\n",
        "    # TensorFlow predictions\n",
        "    tf_preds = tf_model.predict(X_test)\n",
        "\n",
        "    # PyTorch predictions\n",
        "    torch_model.eval()\n",
        "    with torch.no_grad():\n",
        "        torch_preds = torch_model(torch.FloatTensor(X_test))\n",
        "\n",
        "    return X_test, tf_preds, torch_preds.numpy()\n",
        "\n",
        "# Make and display predictions\n",
        "X_test, tf_preds, torch_preds = make_predictions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1AkzH7PA9RS"
      },
      "source": [
        "\n",
        "1. What is a tensor?\n",
        "A tensor is a fundamental data structure used in deep learning - it's a generalization of vectors and matrices to potentially higher dimensions. Think of it as:\n",
        "- Scalar: 0-dimensional tensor (single number)\n",
        "- Vector: 1-dimensional tensor (list of numbers)\n",
        "- Matrix: 2-dimensional tensor (table of numbers)\n",
        "- N-dimensional array: Higher dimensional tensor\n",
        "\n",
        "Example:\n",
        "```python\n",
        "# Scalar (0D tensor)\n",
        "scalar = tf.constant(5)\n",
        "\n",
        "# Vector (1D tensor)\n",
        "vector = tf.constant([1, 2, 3, 4])\n",
        "\n",
        "# Matrix (2D tensor)\n",
        "matrix = tf.constant([[1, 2], [3, 4]])\n",
        "\n",
        "# 3D tensor\n",
        "tensor_3d = tf.constant([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
        "```\n",
        "\n",
        "2. Dimensions and Ranks in TensorFlow:\n",
        "- Rank: Number of dimensions in a tensor\n",
        "- Shape: Length of each dimension\n",
        "\n",
        "Examples:\n",
        "```python\n",
        "# Rank 0 (scalar): shape []\n",
        "t0 = tf.constant(42)\n",
        "\n",
        "# Rank 1 (vector): shape [3]\n",
        "t1 = tf.constant([1, 2, 3])\n",
        "\n",
        "# Rank 2 (matrix): shape [2, 3]\n",
        "t2 = tf.constant([[1, 2, 3],\n",
        "                 [4, 5, 6]])\n",
        "\n",
        "# Rank 3: shape [2, 2, 2]\n",
        "t3 = tf.constant([[[1, 2], [3, 4]],\n",
        "                 [[5, 6], [7, 8]]])\n",
        "```\n",
        "\n",
        "3. Building Models in Keras:\n",
        "There are three main ways to build models in Keras:\n",
        "\n",
        "a. Sequential API (simplest):\n",
        "```python\n",
        "from tensorflow.keras import Sequential, layers\n",
        "\n",
        "model = Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(784,)),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "```\n",
        "\n",
        "b. Functional API (more flexible):\n",
        "```python\n",
        "from tensorflow.keras import Model, Input\n",
        "\n",
        "inputs = Input(shape=(784,))\n",
        "x = layers.Dense(64, activation='relu')(inputs)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "outputs = layers.Dense(10, activation='softmax')(x)\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "```\n",
        "\n",
        "c. Subclassing (most flexible):\n",
        "```python\n",
        "class CustomModel(Model):\n",
        "    def __init__(self):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.dense1 = layers.Dense(64, activation='relu')\n",
        "        self.dropout = layers.Dropout(0.2)\n",
        "        self.dense2 = layers.Dense(10, activation='softmax')\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dropout(x)\n",
        "        return self.dense2(x)\n",
        "```\n",
        "\n",
        "4. Function Operations in Theano:\n",
        "Theano (though now deprecated) introduced several key concepts still used in modern frameworks:\n",
        "- Symbolic Variables: Placeholders for data\n",
        "- Computational Graphs: Operations arranged in a directed graph\n",
        "- Function Compilation: Converting symbolic expressions to efficient code\n",
        "\n",
        "Example of Theano-style operations (modern equivalent in TensorFlow):\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define variables\n",
        "x = tf.Variable(initial_value=3.0)\n",
        "y = tf.Variable(initial_value=2.0)\n",
        "\n",
        "# Define operation\n",
        "@tf.function  # Similar to Theano's function compilation\n",
        "def compute_z(x, y):\n",
        "    return tf.square(x) + y\n",
        "\n",
        "# Execute operation\n",
        "z = compute_z(x, y)\n",
        "```\n",
        "\n",
        "5. PyTorch vs TensorFlow Differences:\n",
        "\n",
        "Key Differences:\n",
        "1. Dynamic vs Static Graphs:\n",
        "```python\n",
        "# PyTorch (Dynamic)\n",
        "class PyTorchModel(nn.Module):\n",
        "    def forward(self, x):\n",
        "        # Can modify behavior at runtime\n",
        "        if self.training:\n",
        "            return x * 2\n",
        "        return x\n",
        "\n",
        "# TensorFlow (Static with @tf.function)\n",
        "@tf.function\n",
        "def tensorflow_model(x):\n",
        "    # Graph is fixed after first run\n",
        "    return x * 2\n",
        "```\n",
        "\n",
        "2. Eager Execution:\n",
        "```python\n",
        "# PyTorch (Always eager by default)\n",
        "x = torch.tensor([1, 2, 3])\n",
        "y = x + 2  # Immediate execution\n",
        "\n",
        "# TensorFlow (Can switch between eager and graph)\n",
        "tf.config.run_functions_eagerly(True)  # Enable eager mode\n",
        "x = tf.constant([1, 2, 3])\n",
        "y = x + 2  # Immediate execution\n",
        "```\n",
        "\n",
        "3. API Design:\n",
        "```python\n",
        "# PyTorch (Object-Oriented)\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(10, 5),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(5, 1)\n",
        ")\n",
        "\n",
        "# TensorFlow (More functional)\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(5, activation='relu', input_shape=(10,)),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "```\n",
        "\n",
        "The main philosophical differences are:\n",
        "- PyTorch is more Python-native and research-friendly\n",
        "- TensorFlow is more production-focused with better deployment tools\n",
        "- PyTorch has a more imperative style\n",
        "- TensorFlow has better visualization tools (TensorBoard)\n",
        "- PyTorch has better debugging capabilities due to its dynamic nature\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
